###imports
import torch
from torch.autograd import Variable
import pandas as pd
import numpy as np
import numpy.random as npr

import sinkhorn_pointcloud

### params
#initialise expansion parameters and time discretisation
epsilon = 0.2 
T = 2 #final time for t0
Tf = (epsilon**2)*T  #final time for t2
h_step = 0.0001 #FOR T=2 use: 0.0001 #0.000025#for histograms: 0.0001

#decimal places for the time lookup.
dps =  int(np.ceil(-np.log10(h_step))+1)
t_steps = int(Tf/h_step) + 1 #number of timesteps
t2_vec = np.round(np.linspace(0,Tf,t_steps,endpoint = True),dps)
h0_step = np.round(h_step/(epsilon**2),4)

times_t0 = np.round(t2_vec/(epsilon**2),4)

#boundary conditions
n = 100  # number of samples in the histograms
niters = 100 #iterations in sinkhorn algorithm

#set up the boundary conditions
peak_center = 1
denom = 1
xmin = -5
xmax = 5

q = np.linspace(xmin,xmax,n) #fixed axes of points
#qnorm = np.linspace(-15,15,50000) #used for computing the normalisation
#qchoice = np.linspace(xmin,xmax,n*100) #points to choose from for histograms


#exact boundary conditions
def p_initial_unnormalised(q):
  return np.exp(-(q-peak_center)**4/denom)
def p_final_unnormalised(q):
  return np.exp(-(((q**2 -peak_center**2)**2)/denom))

#compute normalisation constacomputents
pi_norm = np.abs(np.trapz(p_initial_unnormalised(np.linspace(-8,8,10000)),np.linspace(-8,8,10000)))
pf_norm = np.abs(np.trapz(p_final_unnormalised(np.linspace(-8,8,10000)),np.linspace(-8,8,10000)))

#normalised boundary conds
def p_initial(q):
  return p_initial_unnormalised(q)/pi_norm
def p_final(q):
  return p_final_unnormalised(q)/pf_norm


w2_dist,G0 = sinkhorn_pointcloud.sinkhorn_loss(torch.from_numpy(xs).reshape((n, 1)) , torch.from_numpy(xt).reshape((n, 1)) , 0.1, n, niters)

#find the maximum and replace with ones and return it back to numpy
G0 = np.where(G0.numpy() == row_maxes, 1, 0)

##initialise dataframe
df_orig = pd.DataFrame()

#find lagrangian trajectories and burgers velocities
def get_rho_lambda(i,G0,xt):

  '''input:
  - i: index of the initial coordinate
  - t: time of evaluation

  returns:
  - l_map: approximation of the dynamic lagrangian map between the two distributions as a function of time
  - dsigma_x: dsigma (burger's velocity) evaluated at time (index) and x (l_map)
  '''

  idx_j = 0

  #get matching coordinate
  for j in range(0,n):
    if G0[i,j] > 0:
      idx_j = j
      break

  #get initial and final points
  xinit = xs[i]
  xfinal = xt[idx_j]

  #make (discrete) lagrangian maps
  l_map = np.ones(t_steps)
  for t in range(0,t_steps):
    tcurr = t2_vec[t]
    l_map[t] = ((Tf - tcurr)/Tf)*xinit + (tcurr/Tf)*xfinal

  #get burgers velocity (dsigma)
  dsigma_x = np.ones(t_steps)
  for x in enumerate(l_map):
    #evaluate sigma at x
    dsigma_x[x[0]] = (1/Tf)*(xfinal - xinit)

  return t2_vec,l_map,dsigma_x

for x in enumerate(xs):
  result = get_rho_lambda(x[0],G0,xt)

  #append straight to dataframe

  #make new df with these

  columns=["t","x","dsigma"]

  #append new
  df2append = pd.DataFrame(dict(zip(columns, result)))
  df_orig = pd.concat([df_orig,df2append])
  
##tidy up

#sort by t
df_orig.sort_values(["t","x"],inplace=True)

df_orig = df_orig.reset_index(drop=True)

#round t2 column times to dps decimal places
df_orig['t'] = df_orig['t'].apply(lambda u: round(u, dps))
#round x col
df_orig['x'] = df_orig['x'].apply(lambda u: round(u, 5))

#sort colums
df_orig.sort_values(["t","x"],inplace=True)
df_orig.reset_index(drop=True,inplace=True)

#replace duplicates with mean of values
df_orig = df_orig.groupby(['t','x']).mean().reset_index()

#add new columns
df_orig["ptx"] = 1
df_orig["logptx"] = 1

df = pd.DataFrame()


for t2 in t2_vec:
  xz = df_orig[df_orig.t == t2].x.to_numpy()
  dsigmax = df_orig[df_orig.t == t2].dsigma.to_numpy()

  #run kde on these points
  kde = KernelDensity(kernel='epanechnikov', bandwidth=0.15).fit(xz.reshape(-1, 1))

  #estimated pdf
  logrho_temp = kde.score_samples(x_axis.reshape(-1, 1))
  dens = np.exp(logrho_temp)

  #interpolation of sigma
  interp_dsigma = sci.interp1d(xz,dsigmax, kind='cubic', bounds_error=False, fill_value=(dsigmax[0], dsigmax[-1]), assume_sorted=True)

  #make new df with these
  data= [t2*np.ones(N), x_axis, interp_dsigma(x_axis),logrho_temp, dens]
  columns=["t","x","dsigma","logptx","ptx"]

  #append new
  df2append = pd.DataFrame(dict(zip(columns, data)))
  df_new = pd.concat([df,df2append])

#sort by
df.sort_values(["t","x"],inplace=True)

df = df_new.reset_index(drop=True)


df.to_csv("results_test.csv")
